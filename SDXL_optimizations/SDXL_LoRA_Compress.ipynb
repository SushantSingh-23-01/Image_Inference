{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uBZkVgvMdZ7l",
        "tIjOkKEndc1y",
        "V8jSF6iSoMRN",
        "rs8wWqd_c8rE",
        "81X7bLy1dCjE",
        "IojpPeXOMdHn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Importing Libraries"
      ],
      "metadata": {
        "id": "P2C4uPltdtp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from safetensors.torch import load_file, save_file\n",
        "from dataclasses import dataclass"
      ],
      "metadata": {
        "id": "nVVM5LABr0d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Warning`: Not all loras can be compressed using this technique. (atleast the low step loras in my experimentations).\n",
        "\n",
        "For low step loras in fp32 format, it can be converted to fp16 format. Check the last section for it."
      ],
      "metadata": {
        "id": "kawcvceqKLSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Download Models"
      ],
      "metadata": {
        "id": "Bt-dxMdxFYKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Text Guide"
      ],
      "metadata": {
        "id": "uBZkVgvMdZ7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Note`: Civitai api format for downloading models keeps on chainging. So this might look different in future.\n",
        "\n",
        "Current Format: `https://civitai.com/api/download/models//{VersionID}?token={api_token}`\n",
        "\n",
        "Also, any sdxl finetune will give results (choice is not important)."
      ],
      "metadata": {
        "id": "8x9FizqJEJ04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to find *VersionID* and *API Key*:<br><br>\n",
        "1. **VersionID**: *Model_Page (on CivitAI) -> Details (Right Side) -> AIR (Last colummn) -> VersionID (Last number)*. <br><br>\n",
        "2.**API Keys**: *Civitai_account (Top Right Corner, Profile Picture) -> Account Settings (Bottom) -> API Keys (Scroll to bottom) -> API Keys Add API Key (Copy it and save it somewhere for future use, you can only access it once)*"
      ],
      "metadata": {
        "id": "VYjPWUfIIY7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Version Id and API Token"
      ],
      "metadata": {
        "id": "tIjOkKEndc1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chkpt_VersionId = None #@param {type: 'integer'}\n",
        "lora_VersionId = None #@param {type: 'integer'}\n",
        "civitai_api_token = \"\" #@param {type: 'string'}\n",
        "\n",
        "chkpt_download_path = f\"https://civitai.com/api/download/models/{chkpt_VersionId}?token={civitai_api_token}\"\n",
        "lora_download_path = f\"https://civitai.com/api/download/models/{lora_VersionId}?token={civitai_api_token}\"\n",
        "\n",
        "!wget \"{chkpt_download_path}\" --content-disposition -P \"/content\"\n",
        "!wget \"{lora_download_path}\" --content-disposition -P \"/content\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z0SZS4GdN_WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Setup Environment"
      ],
      "metadata": {
        "id": "V8jSF6iSoMRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Main Github Repository`: https://github.com/elias-gaeros/resize_lora\n",
        "\n",
        "Clone the repository and install necessary libraries."
      ],
      "metadata": {
        "id": "EjQovAasoPwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKci4SMfmYg6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/elias-gaeros/resize_lora.git\n",
        "%cd resize_lora"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Config"
      ],
      "metadata": {
        "id": "V5FzpHVqbdlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Short Parameter Guide"
      ],
      "metadata": {
        "id": "rs8wWqd_c8rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Key Arguments`:\n",
        "\n",
        "- **model**: Path to the input LoRA file.\n",
        "- **output**: Path to save the resized LoRA file.\n",
        "- **new_rank**: The target rank (integer). Set to 0 for dynamic rank selection.\n",
        "- **device**: cpu or cuda (for GPU acceleration, much faster for SVD).\n",
        "- **mode**: svd or extract.\n",
        "- **dynamic_method, dynamic_param**: Used when new_rank=0.\n",
        "- **verbose**: Print more details during processing.\n",
        "\n",
        "`Note`: Refer to orignal repository for more indepth explaination of parameters."
      ],
      "metadata": {
        "id": "-3ryz5ATbfwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Warning`: Put the exact name of downloaded models.\n",
        "Tips:\n",
        "- On the left hand toolbar -> Last icon Files -> Copy name with extension.\n",
        "- Easier way is to hover over lora name -> click on three dots -> Copy path\n",
        "\n",
        "\n",
        "Sometimes the downloaded folders have weird names, thats why it is preferable to manually input the names."
      ],
      "metadata": {
        "id": "uQEY6tS_b02k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Main Config"
      ],
      "metadata": {
        "id": "81X7bLy1dCjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chkpt_path = \"/content/cyberrealisticXL_v53.safetensors\" #@param {type: 'string'}\n",
        "input_lora_path = \"/content/Breckie_Hill-000007.safetensors\" #@param {type: 'string'}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def check_paths(path):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"⚠️ WARNING: Input file '{path}' does not exist.\")\n",
        "        print(\"Please upload your Chkpt/LoRA file and make sure the path is correct.\")\n",
        "    else:\n",
        "        print(f\"✅ Input Chkpt/LoRA found: {path}\")\n",
        "\n",
        "check_paths(chkpt_path)\n",
        "check_paths(input_lora_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zWTc2mTUYyYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Resizing"
      ],
      "metadata": {
        "id": "QuCEMvN5dO77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "command = f\"python resize_lora.py \\\n",
        "            {chkpt_path} \\\n",
        "            {input_lora_path} \\\n",
        "            -o '' \\\n",
        "            -v -r spn_lora=1,thr=-0.7:spn_ckpt=1,thr=-1.2:subspace=0.5,spn_ckpt=0.5,size=32\"\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Executing command:\\n{command}\\n\")\n",
        "print(f\"Using device: {device}\")\n",
        "!{command}\n",
        "\n",
        "print(\"\\nResizing process finished.\")"
      ],
      "metadata": {
        "id": "uv7LTpinYbZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Note`: Dowload File from files in left hand toolbar. There might be multiple versions, download all and test them all in your ui."
      ],
      "metadata": {
        "id": "r7gWoABicFXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Additional: FP32 -> FP16 Conversions"
      ],
      "metadata": {
        "id": "IojpPeXOMdHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some loras provide fp32 versions only. This can be used to convert to fp16."
      ],
      "metadata": {
        "id": "ThGgxcWBMiyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fp32_fp16(input_lora_path, output_lora_path):\n",
        "    fp16_state_dict = {}\n",
        "    conversion_count = 0\n",
        "    skipped_count = 0\n",
        "    original_total_params = 0\n",
        "    original_float_params = 0\n",
        "    state_dict = load_file(input_lora_path, device=\"cpu\")\n",
        "    for key, tensor in state_dict.items():\n",
        "        original_total_params += tensor.numel()\n",
        "        if tensor.dtype == torch.float32 or tensor.dtype == torch.bfloat16:\n",
        "            fp16_state_dict[key] = tensor.to(dtype=torch.float16)\n",
        "            conversion_count += 1\n",
        "            original_float_params += tensor.numel()\n",
        "        else:\n",
        "            # Keep non-float tensors (like int, bool, etc.) as they are\n",
        "            fp16_state_dict[key] = tensor\n",
        "            skipped_count += 1\n",
        "\n",
        "    save_file(fp16_state_dict, output_lora_path)\n",
        "\n",
        "fp32_fp16(f'{input_lora_path}', '/content/compressed_fp16_lora.safetensors')"
      ],
      "metadata": {
        "id": "7TZfz0bKwI0T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}