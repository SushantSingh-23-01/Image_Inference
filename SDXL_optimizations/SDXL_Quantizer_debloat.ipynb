{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "seihJ8G3f3X6",
        "hZBBIOIZX3Sw",
        "jImUIetcPWIi",
        "P37VxDr63LYB",
        "hkts8C5fWzJ6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4c0f692331a467b9ce3dd0a8a3b308b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a128c1707fa4522a603bce8351bf476",
              "IPY_MODEL_76c146394dca478da299eff60eb23df7",
              "IPY_MODEL_0da5b664c06d4581a4e1aa270b91ebcb"
            ],
            "layout": "IPY_MODEL_69a78b355e6444af8da1d2bfd3e20e30"
          }
        },
        "5a128c1707fa4522a603bce8351bf476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c25510e3657497e8104c8e4a958aa31",
            "placeholder": "​",
            "style": "IPY_MODEL_68b236e8c90645c18b83fda82a6d2b12",
            "value": "diffusion_pytorch_model.safetensors: 100%"
          }
        },
        "76c146394dca478da299eff60eb23df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f531dac36bb4e09b1820edafcddffc8",
            "max": 5135149760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60a094db3a414290b573e4151ced246c",
            "value": 5135149760
          }
        },
        "0da5b664c06d4581a4e1aa270b91ebcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65c31c3a4e834d7d9307c5cd272c7989",
            "placeholder": "​",
            "style": "IPY_MODEL_49ec85ad5e244fb1ac571d7b1f6ba6e2",
            "value": " 5.14G/5.14G [00:32&lt;00:00, 241MB/s]"
          }
        },
        "69a78b355e6444af8da1d2bfd3e20e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c25510e3657497e8104c8e4a958aa31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68b236e8c90645c18b83fda82a6d2b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f531dac36bb4e09b1820edafcddffc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a094db3a414290b573e4151ced246c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65c31c3a4e834d7d9307c5cd272c7989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ec85ad5e244fb1ac571d7b1f6ba6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushantSingh-23-01/Image_Inference/blob/main/SDXL_optimizations/SDXL_Quantizer_debloat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize SDXL"
      ],
      "metadata": {
        "id": "Lb6cipyNfzUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspired By: https://civitai.com/articles/10417\n",
        "\n",
        "`Note`: If the unet folder is not available and only merged checkpoint is available, this notebook's first section is useful for seperating the unet and clipvision files."
      ],
      "metadata": {
        "id": "JFsedI3cmcUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace: https://huggingface.co/John6666\n",
        "\n",
        "`Note`: Most of the sdxl, pony, illustrious models are available on this account."
      ],
      "metadata": {
        "id": "0_gLeUP_uXuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "seihJ8G3f3X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from huggingface_hub import hf_hub_download\n",
        "import logging\n",
        "import os\n",
        "from safetensors.torch import load_file, save_file\n",
        "import torch"
      ],
      "metadata": {
        "id": "2hyVHzKGGeAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Config"
      ],
      "metadata": {
        "id": "XQwTaZB1X5NH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Config Parameters`:\n",
        "- **repo_id**: Hugging Face repository.\n",
        "- **filename**: Check the huggingface repository and if there is no *unet* folder or *safetensors* file's name doesn't match, make changes accordingly.\n",
        "- **filename_prefix**: Desired name for the quantized model. Example: *cyberrealistic_xl, epicrealism_xl* etc.\n",
        "- **output_dir**: Directory where quantized model will be stored.\n",
        "- **quant_type**: Type of quantization to use. Example: *Q4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q8* etc.\n"
      ],
      "metadata": {
        "id": "kZP2AEDydgMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Note`: You still would need to download the clipvision models. Download following files:\n",
        "1. *hf_repo_id/text_encoder/model.safetensors* and rename it to *filename_prefix_clip_l*. (~246 Mb)\n",
        "2. *hf_repo_id/text_encoder_2/model.safetensors* and rename it to *filename_prefix_clip_g*. (1.4 Gb)\n",
        "3. Place this both files in clip vision folder of models directory.\n",
        "4. Vae needs to be only downloaded once for SDXL as all versions have same vae in them. It will also be available in *hf_repo_id/vae/diffusion_pytorch_model.safetensors*\n",
        "5. Download it, rename it to *sdxl_vae* and place it in vae folder."
      ],
      "metadata": {
        "id": "wPPPNxgAsz97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class config:\n",
        "    repo_id = 'John6666/cyberrealistic-pony-v85-sdxl'\n",
        "    filename = 'unet/diffusion_pytorch_model.safetensors'\n",
        "    filename_prefix = 'cyberrealistic-sdxl-pony'\n",
        "    output_dir = '/content/components'\n",
        "    quant_type = 'Q5_K_S'"
      ],
      "metadata": {
        "id": "Bu2oLb_wmgcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(config.output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "UYG42KDoXHM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Model"
      ],
      "metadata": {
        "id": "hZBBIOIZX3Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet = hf_hub_download(repo_id=config.repo_id, filename=config.filename)"
      ],
      "metadata": {
        "id": "fVzlEepu-VIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "c4c0f692331a467b9ce3dd0a8a3b308b",
            "5a128c1707fa4522a603bce8351bf476",
            "76c146394dca478da299eff60eb23df7",
            "0da5b664c06d4581a4e1aa270b91ebcb",
            "69a78b355e6444af8da1d2bfd3e20e30",
            "8c25510e3657497e8104c8e4a958aa31",
            "68b236e8c90645c18b83fda82a6d2b12",
            "0f531dac36bb4e09b1820edafcddffc8",
            "60a094db3a414290b573e4151ced246c",
            "65c31c3a4e834d7d9307c5cd272c7989",
            "49ec85ad5e244fb1ac571d7b1f6ba6e2"
          ]
        },
        "outputId": "2f6ace15-172a-486c-9960-428bace5a764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "diffusion_pytorch_model.safetensors:   0%|          | 0.00/5.14G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4c0f692331a467b9ce3dd0a8a3b308b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting up Llama.cpp\n",
        "This cell will set up Llama.cpp need for GGUF conversion."
      ],
      "metadata": {
        "id": "jImUIetcPWIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF0FrDwMncXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05763b99-4c25-4689-b1c2-97ea0f5e16d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 48449, done.\u001b[K\n",
            "remote: Counting objects: 100% (383/383), done.\u001b[K\n",
            "remote: Compressing objects: 100% (269/269), done.\u001b[K\n",
            "remote: Total 48449 (delta 272), reused 120 (delta 114), pack-reused 48066 (from 4)\u001b[K\n",
            "Receiving objects: 100% (48449/48449), 102.88 MiB | 23.89 MiB/s, done.\n",
            "Resolving deltas: 100% (34773/34773), done.\n",
            "Processing ./llama.cpp/gguf-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (6.0.2)\n",
            "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (0.2.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (4.67.1)\n",
            "Building wheels for collected packages: gguf\n",
            "  Building wheel for gguf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gguf: filename=gguf-0.16.0-py3-none-any.whl size=80598 sha256=dd9cd5f887e237d0f99ad345b067c7cd83f731710324fbdc0803b2bbfcef37be\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6c/c6/6dbfb804e7a1607174676026fc9bf5d1006ceff85ba5c680b6\n",
            "Successfully built gguf\n",
            "Installing collected packages: gguf\n",
            "Successfully installed gguf-0.16.0\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "# Clone the llama.cpp repository\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "# Install gguf-py\n",
        "!pip install llama.cpp/gguf-py\n",
        "\n",
        "# Change to the llama.cpp directory\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download conversion files and patching\n",
        "Thsi cell will patch Llama.cpp to recognize the SDXL architecture. The convert.py and the patch are from the city96's repo."
      ],
      "metadata": {
        "id": "P37VxDr63LYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download convert.py\n",
        "!wget -O convert.py \"https://raw.githubusercontent.com/city96/ComfyUI-GGUF/main/tools/convert.py\"\n",
        "\n",
        "# Download convert_g.py for clip_g\n",
        "!wget -O convert_g.py \"https://huggingface.co/Old-Fisherman/SDXL_Finetune_GGUF_Files/resolve/main/convert_g.py\"\n",
        "\n",
        "# Download lcpp patch\n",
        "!wget -O lcpp.patch \"https://raw.githubusercontent.com/city96/ComfyUI-GGUF/main/tools/lcpp.patch\"\n",
        "\n",
        "# Patching lcpp\n",
        "!git checkout tags/b3600\n",
        "!git apply lcpp.patch\n",
        "\n",
        "# Create the build directory\n",
        "!mkdir build\n",
        "\n",
        "# Change to the build directory\n",
        "%cd build\n",
        "\n",
        "# Run cmake to configure the build\n",
        "!cmake ..\n",
        "\n",
        "# Build the target with cmake\n",
        "!cmake --build . --config Debug -j10 --target llama-quantize\n",
        "\n",
        "# Change back to the previous directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "luVaBYtyFXEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39563900-7d68-4701-9d8f-2727753a2b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-11 11:48:51--  https://raw.githubusercontent.com/city96/ComfyUI-GGUF/main/tools/convert.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12007 (12K) [text/plain]\n",
            "Saving to: ‘convert.py’\n",
            "\n",
            "convert.py          100%[===================>]  11.73K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-11 11:48:52 (18.0 MB/s) - ‘convert.py’ saved [12007/12007]\n",
            "\n",
            "--2025-04-11 11:48:52--  https://huggingface.co/Old-Fisherman/SDXL_Finetune_GGUF_Files/resolve/main/convert_g.py\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.12, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5077 (5.0K) [text/plain]\n",
            "Saving to: ‘convert_g.py’\n",
            "\n",
            "convert_g.py        100%[===================>]   4.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-11 11:48:53 (1.84 GB/s) - ‘convert_g.py’ saved [5077/5077]\n",
            "\n",
            "--2025-04-11 11:48:53--  https://raw.githubusercontent.com/city96/ComfyUI-GGUF/main/tools/lcpp.patch\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18434 (18K) [text/plain]\n",
            "Saving to: ‘lcpp.patch’\n",
            "\n",
            "lcpp.patch          100%[===================>]  18.00K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-11 11:48:53 (15.8 MB/s) - ‘lcpp.patch’ saved [18434/18434]\n",
            "\n",
            "Note: switching to 'tags/b3600'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 2fb92678 Fix incorrect use of ctx_split for bias tensors (#9063)\n",
            "error: patch failed: ggml/include/ggml.h:223\n",
            "error: ggml/include/ggml.h: patch does not apply\n",
            "error: patch failed: src/llama.cpp:205\n",
            "error: src/llama.cpp: patch does not apply\n",
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- OpenMP found\n",
            "-- Using llamafile\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done (3.1s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 33%] Built target build_info\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CXX shared library libggml.so\u001b[0m\n",
            "[ 40%] Built target ggml\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX shared library libllama.so\u001b[0m\n",
            "[ 66%] Built target llama\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 93%] Built target common\n",
            "[100%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[100%] Built target llama-quantize\n",
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conversion from fp16 Unet Safetensors to Quant_xxx GGUF"
      ],
      "metadata": {
        "id": "hkts8C5fWzJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Quantizer:\n",
        "    def __init__(self, input_path, output_dir, filename_prefix, quant_type):\n",
        "        self.input_path = input_path\n",
        "        self.output_dir = output_dir\n",
        "        self.filename_prefix = filename_prefix\n",
        "        self.quant_type = quant_type\n",
        "\n",
        "    def convert_fp16_gguf16(self):\n",
        "        dst = os.path.join(config.output_dir, f\"{config.filename_prefix}-F16.gguf\")\n",
        "        if not os.path.exists(self.input_path):\n",
        "            print(f\"Error: Source file not found at {self.input_path}\")\n",
        "        else:\n",
        "            command = f\"python convert.py --src {self.input_path} --dst {dst}\"\n",
        "            print(f\"Running command: {command}\")\n",
        "            !{command}\n",
        "\n",
        "    def convert_gguf16_to_qx(self):\n",
        "        src = os.path.join(config.output_dir, f\"{config.filename_prefix}-F16.gguf\")\n",
        "        dst = os.path.join(self.output_dir, f\"{self.filename_prefix}_{self.quant_type}.gguf\")\n",
        "        !./build/bin/llama-quantize {src} {dst} {config.quant_type}\n",
        "\n",
        "    def __call__(self):\n",
        "        self.convert_fp16_gguf16()\n",
        "        self.convert_gguf16_to_qx()\n",
        "\n",
        "\n",
        "quantizer = Quantizer(unet, config.output_dir, config.filename_prefix, config.quant_type)\n",
        "quantizer()"
      ],
      "metadata": {
        "id": "8zvybMdCSilU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b9bc88-d20a-4e2a-bc1f-a14d1a7f6fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running command: python convert.py --src /root/.cache/huggingface/hub/models--John6666--cyberrealistic-pony-v85-sdxl/snapshots/3e34631448a938e007831dbca9156d6352f3c4f9/unet/diffusion_pytorch_model.safetensors --dst /content/components/cyberrealistic-sdxl-pony-F16.gguf\n",
            "* Architecture detected from input: sdxl\n",
            "add_embedding.linear_1.bias                                               torch.float16 --> F16, shape = {1280}\n",
            "add_embedding.linear_1.weight                                             torch.float16 --> F16, shape = {2816, 1280}\n",
            "add_embedding.linear_2.bias                                               torch.float16 --> F16, shape = {1280}\n",
            "add_embedding.linear_2.weight                                             torch.float16 --> F16, shape = {1280, 1280}\n",
            "conv_in.bias                                                              torch.float16 --> F16, shape = {320}\n",
            "conv_in.weight                                                            torch.float16 --> F16, shape = {256, 45}\n",
            "conv_norm_out.bias                                                        torch.float16 --> F16, shape = {320}\n",
            "conv_norm_out.weight                                                      torch.float16 --> F16, shape = {320}\n",
            "conv_out.bias                                                             torch.float16 --> F16, shape = {4}\n",
            "conv_out.weight                                                           torch.float16 --> F16, shape = {256, 45}\n",
            "down_blocks.0.downsamplers.0.conv.bias                                    torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.downsamplers.0.conv.weight                                  torch.float16 --> F16, shape = {256, 3600}\n",
            "down_blocks.0.resnets.0.conv1.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.0.conv1.weight                                      torch.float16 --> F16, shape = {256, 3600}\n",
            "down_blocks.0.resnets.0.conv2.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.0.conv2.weight                                      torch.float16 --> F16, shape = {256, 3600}\n",
            "down_blocks.0.resnets.0.norm1.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.0.norm1.weight                                      torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.0.norm2.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.0.norm2.weight                                      torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.0.time_emb_proj.bias                                torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.0.time_emb_proj.weight                              torch.float16 --> F16, shape = {1280, 320}\n",
            "down_blocks.0.resnets.1.conv1.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.1.conv1.weight                                      torch.float16 --> F16, shape = {256, 3600}\n",
            "down_blocks.0.resnets.1.conv2.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.1.conv2.weight                                      torch.float16 --> F16, shape = {256, 3600}\n",
            "down_blocks.0.resnets.1.norm1.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.1.norm1.weight                                      torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.1.norm2.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.1.norm2.weight                                      torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.1.time_emb_proj.bias                                torch.float16 --> F16, shape = {320}\n",
            "down_blocks.0.resnets.1.time_emb_proj.weight                              torch.float16 --> F16, shape = {1280, 320}\n",
            "down_blocks.1.attentions.0.norm.bias                                      torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.norm.weight                                    torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.proj_in.bias                                   torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.proj_in.weight                                 torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.proj_out.bias                                  torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.proj_out.weight                                torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias        torch.float16 --> F16, shape = {5120}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight      torch.float16 --> F16, shape = {256, 12800}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias             torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight           torch.float16 --> F16, shape = {2560, 640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias        torch.float16 --> F16, shape = {5120}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight      torch.float16 --> F16, shape = {256, 12800}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias             torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight           torch.float16 --> F16, shape = {2560, 640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.norm.bias                                      torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.norm.weight                                    torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.proj_in.bias                                   torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.proj_in.weight                                 torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.proj_out.bias                                  torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.proj_out.weight                                torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias        torch.float16 --> F16, shape = {5120}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight      torch.float16 --> F16, shape = {256, 12800}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias             torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight           torch.float16 --> F16, shape = {2560, 640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias       torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight     torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight         torch.float16 --> F16, shape = {256, 1600}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias        torch.float16 --> F16, shape = {5120}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight      torch.float16 --> F16, shape = {256, 12800}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias             torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight           torch.float16 --> F16, shape = {2560, 640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight              torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.downsamplers.0.conv.bias                                    torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.downsamplers.0.conv.weight                                  torch.float16 --> F16, shape = {256, 14400}\n",
            "down_blocks.1.resnets.0.conv1.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.0.conv1.weight                                      torch.float16 --> F16, shape = {256, 7200}\n",
            "down_blocks.1.resnets.0.conv2.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.0.conv2.weight                                      torch.float16 --> F16, shape = {256, 14400}\n",
            "down_blocks.1.resnets.0.conv_shortcut.bias                                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.0.conv_shortcut.weight                              torch.float16 --> F16, shape = {256, 800}\n",
            "down_blocks.1.resnets.0.norm1.bias                                        torch.float16 --> F16, shape = {320}\n",
            "down_blocks.1.resnets.0.norm1.weight                                      torch.float16 --> F16, shape = {320}\n",
            "down_blocks.1.resnets.0.norm2.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.0.norm2.weight                                      torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.0.time_emb_proj.bias                                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.0.time_emb_proj.weight                              torch.float16 --> F16, shape = {1280, 640}\n",
            "down_blocks.1.resnets.1.conv1.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.1.conv1.weight                                      torch.float16 --> F16, shape = {256, 14400}\n",
            "down_blocks.1.resnets.1.conv2.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.1.conv2.weight                                      torch.float16 --> F16, shape = {256, 14400}\n",
            "down_blocks.1.resnets.1.norm1.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.1.norm1.weight                                      torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.1.norm2.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.1.norm2.weight                                      torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.1.time_emb_proj.bias                                torch.float16 --> F16, shape = {640}\n",
            "down_blocks.1.resnets.1.time_emb_proj.weight                              torch.float16 --> F16, shape = {1280, 640}\n",
            "down_blocks.2.attentions.0.norm.bias                                      torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.norm.weight                                    torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.proj_in.bias                                   torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.proj_in.weight                                 torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.proj_out.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.proj_out.weight                                torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.norm.bias                                      torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.norm.weight                                    torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.proj_in.bias                                   torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.proj_in.weight                                 torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.proj_out.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.proj_out.weight                                torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias       torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight     torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight         torch.float16 --> F16, shape = {2048, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias        torch.float16 --> F16, shape = {10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight      torch.float16 --> F16, shape = {1280, 10240}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias             torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight           torch.float16 --> F16, shape = {5120, 1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight              torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.0.conv1.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.0.conv1.weight                                      torch.float16 --> F16, shape = {256, 28800}\n",
            "down_blocks.2.resnets.0.conv2.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.0.conv2.weight                                      torch.float16 --> F16, shape = {256, 57600}\n",
            "down_blocks.2.resnets.0.conv_shortcut.bias                                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.0.conv_shortcut.weight                              torch.float16 --> F16, shape = {256, 3200}\n",
            "down_blocks.2.resnets.0.norm1.bias                                        torch.float16 --> F16, shape = {640}\n",
            "down_blocks.2.resnets.0.norm1.weight                                      torch.float16 --> F16, shape = {640}\n",
            "down_blocks.2.resnets.0.norm2.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.0.norm2.weight                                      torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.0.time_emb_proj.bias                                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.0.time_emb_proj.weight                              torch.float16 --> F16, shape = {1280, 1280}\n",
            "down_blocks.2.resnets.1.conv1.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.1.conv1.weight                                      torch.float16 --> F16, shape = {256, 57600}\n",
            "down_blocks.2.resnets.1.conv2.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.1.conv2.weight                                      torch.float16 --> F16, shape = {256, 57600}\n",
            "down_blocks.2.resnets.1.norm1.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.1.norm1.weight                                      torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.1.norm2.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.1.norm2.weight                                      torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.1.time_emb_proj.bias                                torch.float16 --> F16, shape = {1280}\n",
            "down_blocks.2.resnets.1.time_emb_proj.weight                              torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.norm.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.norm.weight                                        torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.proj_in.bias                                       torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.proj_in.weight                                     torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.proj_out.bias                                      torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.proj_out.weight                                    torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.0.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.1.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.2.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.3.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.4.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.5.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.6.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.7.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.8.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias           torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight         torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight             torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight             torch.float16 --> F16, shape = {2048, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias            torch.float16 --> F16, shape = {10240}\n",
            "mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight          torch.float16 --> F16, shape = {1280, 10240}\n",
            "mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias                 torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight               torch.float16 --> F16, shape = {5120, 1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.norm1.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.norm1.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.norm2.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.norm2.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.norm3.bias                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.attentions.0.transformer_blocks.9.norm3.weight                  torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.conv1.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.conv1.weight                                          torch.float16 --> F16, shape = {256, 57600}\n",
            "mid_block.resnets.0.conv2.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.conv2.weight                                          torch.float16 --> F16, shape = {256, 57600}\n",
            "mid_block.resnets.0.norm1.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.norm1.weight                                          torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.norm2.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.norm2.weight                                          torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.time_emb_proj.bias                                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.0.time_emb_proj.weight                                  torch.float16 --> F16, shape = {1280, 1280}\n",
            "mid_block.resnets.1.conv1.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.1.conv1.weight                                          torch.float16 --> F16, shape = {256, 57600}\n",
            "mid_block.resnets.1.conv2.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.1.conv2.weight                                          torch.float16 --> F16, shape = {256, 57600}\n",
            "mid_block.resnets.1.norm1.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.1.norm1.weight                                          torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.1.norm2.bias                                            torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.1.norm2.weight                                          torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.1.time_emb_proj.bias                                    torch.float16 --> F16, shape = {1280}\n",
            "mid_block.resnets.1.time_emb_proj.weight                                  torch.float16 --> F16, shape = {1280, 1280}\n",
            "time_embedding.linear_1.bias                                              torch.float16 --> F16, shape = {1280}\n",
            "time_embedding.linear_1.weight                                            torch.float16 --> F16, shape = {256, 1600}\n",
            "time_embedding.linear_2.bias                                              torch.float16 --> F16, shape = {1280}\n",
            "time_embedding.linear_2.weight                                            torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.norm.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.norm.weight                                      torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.proj_in.bias                                     torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.proj_in.weight                                   torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.proj_out.bias                                    torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.proj_out.weight                                  torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.norm.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.norm.weight                                      torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.proj_in.bias                                     torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.proj_in.weight                                   torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.proj_out.bias                                    torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.proj_out.weight                                  torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.norm.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.norm.weight                                      torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.proj_in.bias                                     torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.proj_in.weight                                   torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.proj_out.bias                                    torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.proj_out.weight                                  torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias         torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight       torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight           torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias          torch.float16 --> F16, shape = {10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight        torch.float16 --> F16, shape = {1280, 10240}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias               torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight             torch.float16 --> F16, shape = {5120, 1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight                torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.0.conv1.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.0.conv1.weight                                        torch.float16 --> F16, shape = {256, 115200}\n",
            "up_blocks.0.resnets.0.conv2.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.0.conv2.weight                                        torch.float16 --> F16, shape = {256, 57600}\n",
            "up_blocks.0.resnets.0.conv_shortcut.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.0.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.0.resnets.0.norm1.bias                                          torch.float16 --> F16, shape = {2560}\n",
            "up_blocks.0.resnets.0.norm1.weight                                        torch.float16 --> F16, shape = {2560}\n",
            "up_blocks.0.resnets.0.norm2.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.0.norm2.weight                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.0.time_emb_proj.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.0.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.resnets.1.conv1.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.1.conv1.weight                                        torch.float16 --> F16, shape = {256, 115200}\n",
            "up_blocks.0.resnets.1.conv2.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.1.conv2.weight                                        torch.float16 --> F16, shape = {256, 57600}\n",
            "up_blocks.0.resnets.1.conv_shortcut.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.1.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.0.resnets.1.norm1.bias                                          torch.float16 --> F16, shape = {2560}\n",
            "up_blocks.0.resnets.1.norm1.weight                                        torch.float16 --> F16, shape = {2560}\n",
            "up_blocks.0.resnets.1.norm2.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.1.norm2.weight                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.1.time_emb_proj.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.1.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.resnets.2.conv1.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.2.conv1.weight                                        torch.float16 --> F16, shape = {256, 86400}\n",
            "up_blocks.0.resnets.2.conv2.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.2.conv2.weight                                        torch.float16 --> F16, shape = {256, 57600}\n",
            "up_blocks.0.resnets.2.conv_shortcut.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.2.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 9600}\n",
            "up_blocks.0.resnets.2.norm1.bias                                          torch.float16 --> F16, shape = {1920}\n",
            "up_blocks.0.resnets.2.norm1.weight                                        torch.float16 --> F16, shape = {1920}\n",
            "up_blocks.0.resnets.2.norm2.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.2.norm2.weight                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.2.time_emb_proj.bias                                  torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.resnets.2.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 1280}\n",
            "up_blocks.0.upsamplers.0.conv.bias                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.0.upsamplers.0.conv.weight                                      torch.float16 --> F16, shape = {256, 57600}\n",
            "up_blocks.1.attentions.0.norm.bias                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.norm.weight                                      torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.proj_in.bias                                     torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.proj_in.weight                                   torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.proj_out.bias                                    torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.proj_out.weight                                  torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias          torch.float16 --> F16, shape = {5120}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight        torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias               torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight             torch.float16 --> F16, shape = {2560, 640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias          torch.float16 --> F16, shape = {5120}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight        torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias               torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight             torch.float16 --> F16, shape = {2560, 640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.norm.bias                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.norm.weight                                      torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.proj_in.bias                                     torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.proj_in.weight                                   torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.proj_out.bias                                    torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.proj_out.weight                                  torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias          torch.float16 --> F16, shape = {5120}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight        torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias               torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight             torch.float16 --> F16, shape = {2560, 640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias          torch.float16 --> F16, shape = {5120}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight        torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias               torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight             torch.float16 --> F16, shape = {2560, 640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.norm.bias                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.norm.weight                                      torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.proj_in.bias                                     torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.proj_in.weight                                   torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.proj_out.bias                                    torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.proj_out.weight                                  torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias          torch.float16 --> F16, shape = {5120}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight        torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias               torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight             torch.float16 --> F16, shape = {2560, 640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias         torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight       torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight           torch.float16 --> F16, shape = {256, 1600}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight           torch.float16 --> F16, shape = {2048, 640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias          torch.float16 --> F16, shape = {5120}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight        torch.float16 --> F16, shape = {256, 12800}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias               torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight             torch.float16 --> F16, shape = {2560, 640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight                torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.0.conv1.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.0.conv1.weight                                        torch.float16 --> F16, shape = {256, 43200}\n",
            "up_blocks.1.resnets.0.conv2.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.0.conv2.weight                                        torch.float16 --> F16, shape = {256, 14400}\n",
            "up_blocks.1.resnets.0.conv_shortcut.bias                                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.0.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 4800}\n",
            "up_blocks.1.resnets.0.norm1.bias                                          torch.float16 --> F16, shape = {1920}\n",
            "up_blocks.1.resnets.0.norm1.weight                                        torch.float16 --> F16, shape = {1920}\n",
            "up_blocks.1.resnets.0.norm2.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.0.norm2.weight                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.0.time_emb_proj.bias                                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.0.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 640}\n",
            "up_blocks.1.resnets.1.conv1.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.1.conv1.weight                                        torch.float16 --> F16, shape = {256, 28800}\n",
            "up_blocks.1.resnets.1.conv2.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.1.conv2.weight                                        torch.float16 --> F16, shape = {256, 14400}\n",
            "up_blocks.1.resnets.1.conv_shortcut.bias                                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.1.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 3200}\n",
            "up_blocks.1.resnets.1.norm1.bias                                          torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.1.resnets.1.norm1.weight                                        torch.float16 --> F16, shape = {1280}\n",
            "up_blocks.1.resnets.1.norm2.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.1.norm2.weight                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.1.time_emb_proj.bias                                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.1.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 640}\n",
            "up_blocks.1.resnets.2.conv1.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.2.conv1.weight                                        torch.float16 --> F16, shape = {256, 21600}\n",
            "up_blocks.1.resnets.2.conv2.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.2.conv2.weight                                        torch.float16 --> F16, shape = {256, 14400}\n",
            "up_blocks.1.resnets.2.conv_shortcut.bias                                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.2.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 2400}\n",
            "up_blocks.1.resnets.2.norm1.bias                                          torch.float16 --> F16, shape = {960}\n",
            "up_blocks.1.resnets.2.norm1.weight                                        torch.float16 --> F16, shape = {960}\n",
            "up_blocks.1.resnets.2.norm2.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.2.norm2.weight                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.2.time_emb_proj.bias                                  torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.resnets.2.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 640}\n",
            "up_blocks.1.upsamplers.0.conv.bias                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.1.upsamplers.0.conv.weight                                      torch.float16 --> F16, shape = {256, 14400}\n",
            "up_blocks.2.resnets.0.conv1.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.0.conv1.weight                                        torch.float16 --> F16, shape = {256, 10800}\n",
            "up_blocks.2.resnets.0.conv2.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.0.conv2.weight                                        torch.float16 --> F16, shape = {256, 3600}\n",
            "up_blocks.2.resnets.0.conv_shortcut.bias                                  torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.0.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 1200}\n",
            "up_blocks.2.resnets.0.norm1.bias                                          torch.float16 --> F16, shape = {960}\n",
            "up_blocks.2.resnets.0.norm1.weight                                        torch.float16 --> F16, shape = {960}\n",
            "up_blocks.2.resnets.0.norm2.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.0.norm2.weight                                        torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.0.time_emb_proj.bias                                  torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.0.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 320}\n",
            "up_blocks.2.resnets.1.conv1.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.1.conv1.weight                                        torch.float16 --> F16, shape = {256, 7200}\n",
            "up_blocks.2.resnets.1.conv2.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.1.conv2.weight                                        torch.float16 --> F16, shape = {256, 3600}\n",
            "up_blocks.2.resnets.1.conv_shortcut.bias                                  torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.1.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 800}\n",
            "up_blocks.2.resnets.1.norm1.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.2.resnets.1.norm1.weight                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.2.resnets.1.norm2.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.1.norm2.weight                                        torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.1.time_emb_proj.bias                                  torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.1.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 320}\n",
            "up_blocks.2.resnets.2.conv1.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.2.conv1.weight                                        torch.float16 --> F16, shape = {256, 7200}\n",
            "up_blocks.2.resnets.2.conv2.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.2.conv2.weight                                        torch.float16 --> F16, shape = {256, 3600}\n",
            "up_blocks.2.resnets.2.conv_shortcut.bias                                  torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.2.conv_shortcut.weight                                torch.float16 --> F16, shape = {256, 800}\n",
            "up_blocks.2.resnets.2.norm1.bias                                          torch.float16 --> F16, shape = {640}\n",
            "up_blocks.2.resnets.2.norm1.weight                                        torch.float16 --> F16, shape = {640}\n",
            "up_blocks.2.resnets.2.norm2.bias                                          torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.2.norm2.weight                                        torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.2.time_emb_proj.bias                                  torch.float16 --> F16, shape = {320}\n",
            "up_blocks.2.resnets.2.time_emb_proj.weight                                torch.float16 --> F16, shape = {1280, 320}\n",
            "100% 1680/1680 [00:01<00:00, 1012.23it/s]\n",
            "Writing: 100% 5.13G/5.13G [00:58<00:00, 88.1Mbyte/s]\n",
            "main: build = 3600 (2fb92678)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/components/cyberrealistic-sdxl-pony-F16.gguf' to '/content/components/cyberrealistic-sdxl-pony_Q5_K_S.gguf' as Q5_K_S\n",
            "llama_model_quantize: failed to quantize: invalid model: tensor 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.' is duplicated\n",
            "main: failed to quantize model from '/content/components/cyberrealistic-sdxl-pony-F16.gguf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgtSm8iss0Y2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}